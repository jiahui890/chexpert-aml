{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading https://files.pythonhosted.org/packages/53/a9/15a5a338160e5e598a0eef385aa29e0ad3a64d62957abdaaf9ed6349c452/tensorflow-2.5.0-cp37-cp37m-win_amd64.whl (422.6MB)\n",
      "Collecting absl-py~=0.10 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/23/47/835652c7e19530973c73c65e652fc53bd05725d5a7cf9bb8706777869c1e/absl_py-0.13.0-py3-none-any.whl (132kB)\n",
      "Collecting termcolor~=1.1.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting grpcio~=1.34.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/91/e8/f55bf28a975f30f3cb4fd31db7c84eb3ae1551628230cafd7cf33b02b732/grpcio-1.34.1-cp37-cp37m-win_amd64.whl (2.9MB)\n",
      "Collecting flatbuffers~=1.12.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/eb/26/712e578c5f14e26ae3314c39a1bdc4eb2ec2f4ddc89b708cf8e0a0d20423/flatbuffers-1.12-py2.py3-none-any.whl\n",
      "Collecting protobuf>=3.9.2 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/fe/b6/0b5fedc572c8771aa4f4916f81ef404203bafbd7461e859c3338316ec016/protobuf-3.17.3-cp37-cp37m-win_amd64.whl (909kB)\n",
      "Collecting google-pasta~=0.2 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl (57kB)\n",
      "Collecting keras-preprocessing~=1.1.2 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/79/4c/7c3275a01e12ef9368a892926ab932b33bb13d55794881e3573482b378a7/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42kB)\n",
      "Collecting tensorflow-estimator<2.6.0,>=2.5.0rc0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/ec/78/b27f73e923becc6e79e18fe112cf75e3200d1ee35b0dba8fa46181bce56c/tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462kB)\n",
      "Collecting keras-nightly~=2.5.0.dev (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/aa/e7/53bc896aa4e11a87aac10a625c676b3a3d57d1c8d9929e4809d31fa0b7d5/keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2MB)\n",
      "Collecting gast==0.4.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/b6/48/583c032b79ae5b3daa02225a675aeb673e58d2cb698e78510feceb11958c/gast-0.4.0-py3-none-any.whl\n",
      "Collecting opt-einsum~=3.3.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/bc/19/404708a7e54ad2798907210462fd950c3442ea51acc8790f3da48d2bee8b/opt_einsum-3.3.0-py3-none-any.whl (65kB)\n",
      "Collecting wrapt~=1.12.1 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/82/f7/e43cefbe88c5fd371f4cf0cf5eb3feccd07515af9fd6cf7dbf1d1793a797/wrapt-1.12.1.tar.gz\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\arnold\\anaconda3\\lib\\site-packages (from tensorflow) (3.7.4.3)\n",
      "Collecting h5py~=3.1.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/53/c2/77bd81922264520b492bd7bfd1a51a845bc1187445408a7a83db284fd566/h5py-3.1.0-cp37-cp37m-win_amd64.whl (2.7MB)\n",
      "Collecting numpy~=1.19.2 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/ff/18/60ac053857fb924b0324c81200b59c00317ebaa3c14b7478266b50ffed19/numpy-1.19.5-cp37-cp37m-win_amd64.whl (13.2MB)\n",
      "Collecting six~=1.15.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\n",
      "Collecting tensorboard~=2.5 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/44/f5/7feea02a3fb54d5db827ac4b822a7ba8933826b36de21880518250b8733a/tensorboard-2.5.0-py3-none-any.whl (6.0MB)\n",
      "Collecting wheel~=0.35 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/65/63/39d04c74222770ed1589c0eaba06c05891801219272420b40311cd60c880/wheel-0.36.2-py2.py3-none-any.whl\n",
      "Collecting astunparse~=1.6.3 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl\n",
      "Collecting cached-property; python_version < \"3.8\" (from h5py~=3.1.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/48/19/f2090f7dad41e225c7f2326e4cfe6fff49e57dedb5b53636c9551f86b069/cached_property-1.5.2-py2.py3-none-any.whl\n",
      "Collecting google-auth<2,>=1.6.3 (from tensorboard~=2.5->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/6e/1d/e4718af587967b8fb6ac7e0e257809934005a3f8fde8f31a304c7e682874/google_auth-1.32.1-py2.py3-none-any.whl (147kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard~=2.5->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/9d/d3/7541e89f1fc456eef157224f597a8bba22589db6369a03eaba68c11f07a0/google_auth_oauthlib-0.4.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\arnold\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (0.16.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\arnold\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (2.22.0)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard~=2.5->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/74/69/5747a957f95e2e1d252ca41476ae40ce79d70d38151d2e494feb7722860c/tensorboard_data_server-0.6.1-py3-none-any.whl\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.5->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/6e/33/1ae0f71395e618d6140fbbc9587cc3156591f748226075e0f7d6f9176522/Markdown-3.3.4-py3-none-any.whl (97kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\arnold\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (41.4.0)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard~=2.5->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/1a/c1/499e600ba0c618b451cd9c425ae1c177249940a2086316552fee7d86c954/tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781kB)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in c:\\users\\arnold\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\arnold\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\arnold\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.2.8)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\arnold\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\arnold\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arnold\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2019.9.11)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\arnold\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (1.24.2)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\users\\arnold\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow) (0.23)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\arnold\\anaconda3\\lib\\site-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/e8/5d/9dd1c29e5a786525f6342f6c1d812ed2e37edc653ad297048c1668988053/oauthlib-3.1.1-py2.py3-none-any.whl (146kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\arnold\\anaconda3\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\arnold\\anaconda3\\lib\\site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow) (7.2.0)\n",
      "Building wheels for collected packages: termcolor, wrapt\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-cp37-none-any.whl size=4835 sha256=4b48a6064378da72cdcd13fc124d11bc37d644c009842f0857ca69ffd2700bb7\n",
      "  Stored in directory: C:\\Users\\Arnold\\AppData\\Local\\pip\\Cache\\wheels\\7c\\06\\54\\bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "  Building wheel for wrapt (setup.py): started\n",
      "  Building wheel for wrapt (setup.py): finished with status 'done'\n",
      "  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-none-any.whl size=19560 sha256=8494df7c6ecbd03a1d9756837ce60a82980b59c833c76cbdf3e88e791ef11ca5\n",
      "  Stored in directory: C:\\Users\\Arnold\\AppData\\Local\\pip\\Cache\\wheels\\b1\\c2\\ed\\d62208260edbd3fa7156545c00ef966f45f2063d0a84f8208a\n",
      "Successfully built termcolor wrapt\n",
      "Installing collected packages: six, absl-py, termcolor, grpcio, flatbuffers, protobuf, google-pasta, numpy, keras-preprocessing, tensorflow-estimator, keras-nightly, gast, opt-einsum, wrapt, cached-property, h5py, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard-data-server, markdown, wheel, tensorboard-plugin-wit, tensorboard, astunparse, tensorflow\n",
      "  Found existing installation: six 1.12.0\n",
      "    Uninstalling six-1.12.0:\n",
      "      Successfully uninstalled six-1.12.0\n",
      "  Found existing installation: numpy 1.16.5\n",
      "    Uninstalling numpy-1.16.5:\n",
      "      Successfully uninstalled numpy-1.16.5\n",
      "  Found existing installation: wrapt 1.11.2\n",
      "    Uninstalling wrapt-1.11.2:\n",
      "      Successfully uninstalled wrapt-1.11.2\n",
      "  Found existing installation: h5py 2.9.0\n",
      "    Uninstalling h5py-2.9.0:\n",
      "      Successfully uninstalled h5py-2.9.0\n",
      "  Found existing installation: google-auth 1.4.2\n",
      "    Uninstalling google-auth-1.4.2:\n",
      "      Successfully uninstalled google-auth-1.4.2\n",
      "  Found existing installation: wheel 0.33.6\n",
      "    Uninstalling wheel-0.33.6:\n",
      "      Successfully uninstalled wheel-0.33.6\n",
      "Successfully installed absl-py-0.13.0 astunparse-1.6.3 cached-property-1.5.2 flatbuffers-1.12 gast-0.4.0 google-auth-1.32.1 google-auth-oauthlib-0.4.4 google-pasta-0.2.0 grpcio-1.34.1 h5py-3.1.0 keras-nightly-2.5.0.dev2021032900 keras-preprocessing-1.1.2 markdown-3.3.4 numpy-1.19.5 oauthlib-3.1.1 opt-einsum-3.3.0 protobuf-3.17.3 requests-oauthlib-1.3.0 six-1.15.0 tensorboard-2.5.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.5.0 tensorflow-estimator-2.5.0 termcolor-1.1.0 wheel-0.36.2 wrapt-1.12.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: astroid 2.3.1 has requirement six==1.12, but you'll have six 1.15.0 which is incompatible.\n",
      "ERROR: astroid 2.3.1 has requirement wrapt==1.11.*, but you'll have wrapt 1.12.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "#pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for general\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import torch\n",
    "#for image reading and preprocessing\n",
    "import cv2\n",
    "# for model building\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Activation, Add, AveragePooling2D, BatchNormalization, Conv2D, Dense, Dropout, Flatten, GlobalMaxPooling2D, Input, Lambda, MaxPooling2D, MaxPool2D, ZeroPadding2D\n",
    "\n",
    "# for visualising images\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, load in the labels, and paths for image files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"CheXpert-v1.0-small/train.csv\")\n",
    "train.columns = train.columns.str.lower().str.replace(\" \",\"_\").str.replace(\"/\",\"_or_\") #clean col names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arnold\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "train['cropped_path'] = train['path'].str.replace(\"CheXpert-v1.0-small/train/\",\"cropped_equalized/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the distribution of labels in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of conditions\n",
    "conditions = ['no_finding', 'enlarged_cardiomediastinum', 'cardiomegaly', 'lung_opacity',\n",
    "              'lung_lesion', 'edema', 'consolidation', 'pneumonia', \n",
    "              'atelectasis', 'pneumothorax', 'pleural_effusion', 'pleural_other', \n",
    "              'fracture', 'support_devices']\n",
    "\n",
    "#clean up the labels (blank for unmentioned, 0 for negative, -1 for uncertain, and 1 for positive)\n",
    "#we convert unmentioned to negative\n",
    "for col in train.columns:\n",
    "    if col in conditions:\n",
    "        train[col] = train[col].replace(np.nan,0)\n",
    "\n",
    "#generate description of the data\n",
    "train_desc = []\n",
    "\n",
    "for col in train.columns:\n",
    "    if col in conditions:\n",
    "        if -1 in train[col].unique():\n",
    "            train_desc.append([col,\n",
    "                               str(train[col].groupby(train[col]).count()[1])+\n",
    "                               ' ('+\n",
    "                               str(round(train[col].groupby(train[col]).count()[1]/\n",
    "                                         sum(train[col].groupby(train[col]).count())*100,2))+\n",
    "                               '%)',\n",
    "                               str(train[col].groupby(train[col]).count()[-1])+\n",
    "                               ' ('+\n",
    "                               str(round(train[col].groupby(train[col]).count()[-1]/\n",
    "                                         sum(train[col].groupby(train[col]).count())*100,2))+\n",
    "                               '%)',\n",
    "                               str(train[col].groupby(train[col]).count()[0])+\n",
    "                               ' ('+\n",
    "                               str(round(train[col].groupby(train[col]).count()[0]/\n",
    "                                         sum(train[col].groupby(train[col]).count())*100,2))+\n",
    "                               '%)'])\n",
    "        else:\n",
    "            train_desc.append([col,\n",
    "                               str(train[col].groupby(train[col]).count()[1])+\n",
    "                               ' ('+\n",
    "                               str(round(train[col].groupby(train[col]).count()[1]/\n",
    "                                         sum(train[col].groupby(train[col]).count())*100,2))+\n",
    "                               '%)',\n",
    "                               0,\n",
    "                               str(train[col].groupby(train[col]).count()[0])+\n",
    "                               ' ('+\n",
    "                               str(round(train[col].groupby(train[col]).count()[0]/\n",
    "                                         sum(train[col].groupby(train[col]).count())*100,2))+\n",
    "                               '%)'])\n",
    "\n",
    "train_desc = pd.DataFrame(train_desc, columns = ['Pathology','Positive','Uncertain','Negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223414"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train[col].groupby(train[col]).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pathology</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Uncertain</th>\n",
       "      <th>Negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no_finding</td>\n",
       "      <td>22381 (10.02%)</td>\n",
       "      <td>0</td>\n",
       "      <td>201033 (89.98%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>enlarged_cardiomediastinum</td>\n",
       "      <td>10798 (4.83%)</td>\n",
       "      <td>12403 (5.55%)</td>\n",
       "      <td>200213 (89.62%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cardiomegaly</td>\n",
       "      <td>27000 (12.09%)</td>\n",
       "      <td>8087 (3.62%)</td>\n",
       "      <td>188327 (84.3%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lung_opacity</td>\n",
       "      <td>105581 (47.26%)</td>\n",
       "      <td>5598 (2.51%)</td>\n",
       "      <td>112235 (50.24%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lung_lesion</td>\n",
       "      <td>9186 (4.11%)</td>\n",
       "      <td>1488 (0.67%)</td>\n",
       "      <td>212740 (95.22%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>edema</td>\n",
       "      <td>52246 (23.39%)</td>\n",
       "      <td>12984 (5.81%)</td>\n",
       "      <td>158184 (70.8%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>consolidation</td>\n",
       "      <td>14783 (6.62%)</td>\n",
       "      <td>27742 (12.42%)</td>\n",
       "      <td>180889 (80.97%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pneumonia</td>\n",
       "      <td>6039 (2.7%)</td>\n",
       "      <td>18770 (8.4%)</td>\n",
       "      <td>198605 (88.9%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>atelectasis</td>\n",
       "      <td>33376 (14.94%)</td>\n",
       "      <td>33739 (15.1%)</td>\n",
       "      <td>156299 (69.96%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pneumothorax</td>\n",
       "      <td>19448 (8.7%)</td>\n",
       "      <td>3145 (1.41%)</td>\n",
       "      <td>200821 (89.89%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pleural_effusion</td>\n",
       "      <td>86187 (38.58%)</td>\n",
       "      <td>11628 (5.2%)</td>\n",
       "      <td>125599 (56.22%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pleural_other</td>\n",
       "      <td>3523 (1.58%)</td>\n",
       "      <td>2653 (1.19%)</td>\n",
       "      <td>217238 (97.24%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>fracture</td>\n",
       "      <td>9040 (4.05%)</td>\n",
       "      <td>642 (0.29%)</td>\n",
       "      <td>213732 (95.67%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>support_devices</td>\n",
       "      <td>116001 (51.92%)</td>\n",
       "      <td>1079 (0.48%)</td>\n",
       "      <td>106334 (47.6%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Pathology         Positive       Uncertain  \\\n",
       "0                   no_finding   22381 (10.02%)               0   \n",
       "1   enlarged_cardiomediastinum    10798 (4.83%)   12403 (5.55%)   \n",
       "2                 cardiomegaly   27000 (12.09%)    8087 (3.62%)   \n",
       "3                 lung_opacity  105581 (47.26%)    5598 (2.51%)   \n",
       "4                  lung_lesion     9186 (4.11%)    1488 (0.67%)   \n",
       "5                        edema   52246 (23.39%)   12984 (5.81%)   \n",
       "6                consolidation    14783 (6.62%)  27742 (12.42%)   \n",
       "7                    pneumonia      6039 (2.7%)    18770 (8.4%)   \n",
       "8                  atelectasis   33376 (14.94%)   33739 (15.1%)   \n",
       "9                 pneumothorax     19448 (8.7%)    3145 (1.41%)   \n",
       "10            pleural_effusion   86187 (38.58%)    11628 (5.2%)   \n",
       "11               pleural_other     3523 (1.58%)    2653 (1.19%)   \n",
       "12                    fracture     9040 (4.05%)     642 (0.29%)   \n",
       "13             support_devices  116001 (51.92%)    1079 (0.48%)   \n",
       "\n",
       "           Negative  \n",
       "0   201033 (89.98%)  \n",
       "1   200213 (89.62%)  \n",
       "2    188327 (84.3%)  \n",
       "3   112235 (50.24%)  \n",
       "4   212740 (95.22%)  \n",
       "5    158184 (70.8%)  \n",
       "6   180889 (80.97%)  \n",
       "7    198605 (88.9%)  \n",
       "8   156299 (69.96%)  \n",
       "9   200821 (89.89%)  \n",
       "10  125599 (56.22%)  \n",
       "11  217238 (97.24%)  \n",
       "12  213732 (95.67%)  \n",
       "13   106334 (47.6%)  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract a random subset of 1000 images to train toy model and another random subset of 300 for validation. Ensure that they don't overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to check training data is not in the validation set\n",
    "def check_train(df, df2):\n",
    "    for row in df.index:\n",
    "        if row in list(df2.index):\n",
    "            return False\n",
    "        else:\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set a standard random seed for use for all random number generation\n",
    "random_seed = 2021\n",
    "\n",
    "#random sample for toy model\n",
    "train_subset = train.sample(5000, random_state = random_seed)\n",
    "\n",
    "#random sample for validation data\n",
    "valid_subset = train.sample(300, random_state = 1000)\n",
    "valid_subset['mask'] = check_train(valid_subset, train_subset)\n",
    "valid_subset = valid_subset[valid_subset['mask']].drop(columns = 'mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create train image set\n",
    "train_data = []\n",
    "for path in train_subset['cropped_path']:\n",
    "    train_data.append(cv2.imread(path, cv2.IMREAD_UNCHANGED))\n",
    "\n",
    "#create validation image set\n",
    "valid_data = []\n",
    "for path in valid_subset['cropped_path']:\n",
    "    valid_data.append(cv2.imread(path, cv2.IMREAD_UNCHANGED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert train_data to np array for the CNN input\n",
    "train_data = np.array(train_data)\n",
    "\n",
    "#convert valid_data to np array for the CNN input\n",
    "valid_data = np.array(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  1,   1,   1, ...,   1,   1,   2],\n",
       "        [  1,   1,   1, ...,   6,   3,   3],\n",
       "        [  1,   1,   1, ...,   3,   2,   0],\n",
       "        ...,\n",
       "        [145, 150, 144, ...,   6,   3,   7],\n",
       "        [139, 157, 156, ...,   2,   0,   0],\n",
       "        [150, 159, 153, ...,   3,   1,   5]],\n",
       "\n",
       "       [[  6,   2,   5, ...,   5,   5,   5],\n",
       "        [  4,   4,   0, ...,   5,   5,   5],\n",
       "        [  1,   3,   2, ...,   5,   5,   5],\n",
       "        ...,\n",
       "        [214, 216, 217, ...,  53,  41,  46],\n",
       "        [216, 217, 219, ...,  77,  78,  86],\n",
       "        [215, 217, 218, ..., 121, 109, 113]],\n",
       "\n",
       "       [[  7,   7,   7, ...,   7,   7,   7],\n",
       "        [  7,   7,   7, ...,   7,   7,   7],\n",
       "        [  7,   7,   7, ...,   7,   7,   7],\n",
       "        ...,\n",
       "        [  0,  37,  24, ...,   1,   2,   0],\n",
       "        [  3,  25,  21, ...,   0,   0,   2],\n",
       "        [  0,  23,  26, ...,   0,   2,   0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[  7,   7,   8, ...,   6,   7,   8],\n",
       "        [  8,   7,   6, ...,   4,   5,   5],\n",
       "        [  7,   7,   8, ...,   4,   4,   4],\n",
       "        ...,\n",
       "        [  9,  10,   9, ...,  11,  11,  11],\n",
       "        [  9,  10,   9, ...,  11,  11,  11],\n",
       "        [  9,  10,   9, ...,  11,  11,  11]],\n",
       "\n",
       "       [[ 52,  55,  59, ...,  28,  28,  30],\n",
       "        [ 52,  49,  56, ...,  43,  52,  35],\n",
       "        [ 47,  52,  60, ...,  43,  36,  43],\n",
       "        ...,\n",
       "        [ 29,  44,  36, ...,  22,  48,  18],\n",
       "        [ 45,  44,  38, ...,  22,  24,  52],\n",
       "        [ 30,  38,  45, ...,  44,  37,  40]],\n",
       "\n",
       "       [[176, 172, 135, ...,  99, 115, 145],\n",
       "        [173, 153, 142, ..., 100, 123, 152],\n",
       "        [175, 154, 140, ...,  93, 118, 144],\n",
       "        ...,\n",
       "        [255, 255, 255, ...,  21,   5,  13],\n",
       "        [255, 255, 255, ...,  27,   5,  13],\n",
       "        [255, 255, 255, ...,  39,  14,   8]]], dtype=uint8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.99215686, -0.99215686, -0.99215686, ..., -0.99215686,\n",
       "         -0.99215686, -0.98431373],\n",
       "        [-0.99215686, -0.99215686, -0.99215686, ..., -0.95294118,\n",
       "         -0.97647059, -0.97647059],\n",
       "        [-0.99215686, -0.99215686, -0.99215686, ..., -0.97647059,\n",
       "         -0.98431373, -1.        ],\n",
       "        ...,\n",
       "        [ 0.1372549 ,  0.17647059,  0.12941176, ..., -0.95294118,\n",
       "         -0.97647059, -0.94509804],\n",
       "        [ 0.09019608,  0.23137255,  0.22352941, ..., -0.98431373,\n",
       "         -1.        , -1.        ],\n",
       "        [ 0.17647059,  0.24705882,  0.2       , ..., -0.97647059,\n",
       "         -0.99215686, -0.96078431]],\n",
       "\n",
       "       [[-0.95294118, -0.98431373, -0.96078431, ..., -0.96078431,\n",
       "         -0.96078431, -0.96078431],\n",
       "        [-0.96862745, -0.96862745, -1.        , ..., -0.96078431,\n",
       "         -0.96078431, -0.96078431],\n",
       "        [-0.99215686, -0.97647059, -0.98431373, ..., -0.96078431,\n",
       "         -0.96078431, -0.96078431],\n",
       "        ...,\n",
       "        [ 0.67843137,  0.69411765,  0.70196078, ..., -0.58431373,\n",
       "         -0.67843137, -0.63921569],\n",
       "        [ 0.69411765,  0.70196078,  0.71764706, ..., -0.39607843,\n",
       "         -0.38823529, -0.3254902 ],\n",
       "        [ 0.68627451,  0.70196078,  0.70980392, ..., -0.05098039,\n",
       "         -0.14509804, -0.11372549]],\n",
       "\n",
       "       [[-0.94509804, -0.94509804, -0.94509804, ..., -0.94509804,\n",
       "         -0.94509804, -0.94509804],\n",
       "        [-0.94509804, -0.94509804, -0.94509804, ..., -0.94509804,\n",
       "         -0.94509804, -0.94509804],\n",
       "        [-0.94509804, -0.94509804, -0.94509804, ..., -0.94509804,\n",
       "         -0.94509804, -0.94509804],\n",
       "        ...,\n",
       "        [-1.        , -0.70980392, -0.81176471, ..., -0.99215686,\n",
       "         -0.98431373, -1.        ],\n",
       "        [-0.97647059, -0.80392157, -0.83529412, ..., -1.        ,\n",
       "         -1.        , -0.98431373],\n",
       "        [-1.        , -0.81960784, -0.79607843, ..., -1.        ,\n",
       "         -0.98431373, -1.        ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.94509804, -0.94509804, -0.9372549 , ..., -0.95294118,\n",
       "         -0.94509804, -0.9372549 ],\n",
       "        [-0.9372549 , -0.94509804, -0.95294118, ..., -0.96862745,\n",
       "         -0.96078431, -0.96078431],\n",
       "        [-0.94509804, -0.94509804, -0.9372549 , ..., -0.96862745,\n",
       "         -0.96862745, -0.96862745],\n",
       "        ...,\n",
       "        [-0.92941176, -0.92156863, -0.92941176, ..., -0.91372549,\n",
       "         -0.91372549, -0.91372549],\n",
       "        [-0.92941176, -0.92156863, -0.92941176, ..., -0.91372549,\n",
       "         -0.91372549, -0.91372549],\n",
       "        [-0.92941176, -0.92156863, -0.92941176, ..., -0.91372549,\n",
       "         -0.91372549, -0.91372549]],\n",
       "\n",
       "       [[-0.59215686, -0.56862745, -0.5372549 , ..., -0.78039216,\n",
       "         -0.78039216, -0.76470588],\n",
       "        [-0.59215686, -0.61568627, -0.56078431, ..., -0.6627451 ,\n",
       "         -0.59215686, -0.7254902 ],\n",
       "        [-0.63137255, -0.59215686, -0.52941176, ..., -0.6627451 ,\n",
       "         -0.71764706, -0.6627451 ],\n",
       "        ...,\n",
       "        [-0.77254902, -0.65490196, -0.71764706, ..., -0.82745098,\n",
       "         -0.62352941, -0.85882353],\n",
       "        [-0.64705882, -0.65490196, -0.70196078, ..., -0.82745098,\n",
       "         -0.81176471, -0.59215686],\n",
       "        [-0.76470588, -0.70196078, -0.64705882, ..., -0.65490196,\n",
       "         -0.70980392, -0.68627451]],\n",
       "\n",
       "       [[ 0.38039216,  0.34901961,  0.05882353, ..., -0.22352941,\n",
       "         -0.09803922,  0.1372549 ],\n",
       "        [ 0.35686275,  0.2       ,  0.11372549, ..., -0.21568627,\n",
       "         -0.03529412,  0.19215686],\n",
       "        [ 0.37254902,  0.20784314,  0.09803922, ..., -0.27058824,\n",
       "         -0.0745098 ,  0.12941176],\n",
       "        ...,\n",
       "        [ 1.        ,  1.        ,  1.        , ..., -0.83529412,\n",
       "         -0.96078431, -0.89803922],\n",
       "        [ 1.        ,  1.        ,  1.        , ..., -0.78823529,\n",
       "         -0.96078431, -0.89803922],\n",
       "        [ 1.        ,  1.        ,  1.        , ..., -0.69411765,\n",
       "         -0.89019608, -0.9372549 ]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize pixel values between -1 and 1\n",
    "train_data = train_data/127.5 - 1\n",
    "valid_data = valid_data/127.5 - 1\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the train and validation labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array(train_subset[conditions])\n",
    "\n",
    "valid_labels = np.array(valid_subset[conditions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decide how to treat the 'uncertain' label\n",
    "#uncertain change to positive\n",
    "train_labels_pos = np.array(train_subset[conditions].replace(-1,1))\n",
    "valid_labels_pos = np.array(valid_subset[conditions].replace(-1,1))\n",
    "\n",
    "#uncertain change to negative\n",
    "train_labels_neg = np.array(train_subset[conditions].replace(-1,1))\n",
    "valid_labels_neg = np.array(valid_subset[conditions].replace(-1,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up and train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall build a multi-label classifier as opposed to a multi-output classifier. \n",
    "\n",
    "[NOTE] Should we consider trying both approaches? I'm worried that the multi-output classifier will take too long to train and tune as it is 14 models.\n",
    "\n",
    "Credits for the pipeline go to https://towardsdatascience.com/building-a-multi-output-convolutional-neural-network-with-keras-ed24c7bc1178"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to create multi-label model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_CNN_model(height,width,classes):\n",
    "\n",
    "    #define input layer\n",
    "    inputs = Input(shape = (height,width,1))\n",
    "\n",
    "    #create the convolution/pooling layers\n",
    "    #Conv2D -> BatchNormalization -> Pooling -> Dropout\n",
    "\n",
    "    x = Conv2D(16, (3, 3), padding=\"same\")(inputs)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = MaxPooling2D(pool_size=(3, 3))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "\n",
    "    x = Conv2D(32, (3, 3), padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "\n",
    "    x = Conv2D(32, (3, 3), padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    \n",
    "    x = Conv2D(32, (3, 3), padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    \n",
    "    x = Conv2D(32, (3, 3), padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    \n",
    "    #create classification layers, first flatten the convolution output\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    #create hidden layers for classification\n",
    "    x = Dense(128)(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    #create output layer\n",
    "    x = Dense(len(classes))(x)\n",
    "    x = Activation(\"sigmoid\", name = 'predicted_observations')(x) #sigmoid and not softmax because we are doing multi-label\n",
    "    \n",
    "    #create model class\n",
    "    model = Model(inputs=inputs,\n",
    "                  outputs = x)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to create ResNet152 model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import glorot_uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to create identity blocks\n",
    "def identity_block(X, f, filters, stage, block):\n",
    "   \n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    F1, F2, F3 = filters\n",
    "\n",
    "    X_shortcut = X\n",
    "   \n",
    "    X = Conv2D(filters=F1, kernel_size=(1, 1), strides=(1, 1), padding='valid', name=conv_name_base + '2a', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same', name=conv_name_base + '2b', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid', name=conv_name_base + '2c', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)\n",
    "\n",
    "    X = Add()([X, X_shortcut])# SKIP Connection\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to create convolutional blocks\n",
    "def convolutional_block(X, f, filters, stage, block, s=2):\n",
    "   \n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    F1, F2, F3 = filters\n",
    "\n",
    "    X_shortcut = X\n",
    "\n",
    "    X = Conv2D(filters=F1, kernel_size=(1, 1), strides=(s, s), padding='valid', name=conv_name_base + '2a', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same', name=conv_name_base + '2b', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid', name=conv_name_base + '2c', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)\n",
    "\n",
    "    X_shortcut = Conv2D(filters=F3, kernel_size=(1, 1), strides=(s, s), padding='valid', name=conv_name_base + '1', kernel_initializer=glorot_uniform(seed=0))(X_shortcut)\n",
    "    X_shortcut = BatchNormalization(axis=3, name=bn_name_base + '1')(X_shortcut)\n",
    "\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to create resnet model\n",
    "def create_resnet50_model(height,width,classes):\n",
    "    \n",
    "    #define input layer\n",
    "    inputs = Input(shape = (height, width, 1))\n",
    "    \n",
    "    #define resnet layers\n",
    "    x = ZeroPadding2D((3, 3))(inputs)\n",
    "\n",
    "    x = Conv2D(64, (7, 7), strides=(2, 2), name='conv1', kernel_initializer=glorot_uniform(seed=0))(x)\n",
    "    x = BatchNormalization(axis=3, name='bn_conv1')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    x = convolutional_block(x, f=3, filters=[64, 64, 256], stage=2, block='a', s=1)\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b')\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='c')\n",
    "\n",
    "\n",
    "    x = convolutional_block(x, f=3, filters=[128, 128, 512], stage=3, block='a', s=2)\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='d')\n",
    "\n",
    "    x = convolutional_block(x, f=3, filters=[256, 256, 1024], stage=4, block='a', s=2)\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='b')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='c')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='d')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='e')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='f')\n",
    "\n",
    "    x = x = convolutional_block(x, f=3, filters=[512, 512, 2048], stage=5, block='a', s=2)\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b')\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c')\n",
    "\n",
    "    x = AveragePooling2D(pool_size=(2, 2), padding='same')(x)\n",
    "    \n",
    "    #create classification layers, first flatten the convolution output\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    #create hidden layers for classification\n",
    "    x = Dense(128)(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    #create output layer\n",
    "    x = Dense(len(classes))(x)\n",
    "    x = Activation(\"sigmoid\", name = 'predicted_observations')(x) #sigmoid and not softmax because we are doing multi-label\n",
    "    \n",
    "    model = Model(inputs=inputs, \n",
    "                  outputs=x, \n",
    "                  name='ResNet50')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to create DenseNet121 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.densenet import DenseNet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_densenet_model(height,width,classes):\n",
    "    #define input layer\n",
    "    inputs = Input(shape = (height,width,1))\n",
    "    \n",
    "    #create the ResNet152 layers\n",
    "    x = DenseNet121(include_top = False,\n",
    "                    input_tensor = inputs)\n",
    "    \n",
    "    #create classification layers, first flatten the convolution output\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    #create hidden layers for classification\n",
    "    x = Dense(128)(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    #create output layer\n",
    "    x = Dense(len(classes))(x)\n",
    "    x = Activation(\"sigmoid\", name = 'predicted_observations')(x) #sigmoid and not softmax because we are doing multi-label\n",
    "    \n",
    "    #create model class\n",
    "    model = Model(inputs=inputs,\n",
    "                  outputs = x)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the type of model and initialise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise the model\n",
    "model = create_resnet50_model(train_data.shape[1],train_data.shape[2],conditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define optimizer for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to do it in this specific way to access the keras API.\n",
    "from keras.optimizers import adam_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_lr = 1e-4\n",
    "epochs = 5\n",
    "\n",
    "opt = adam_v2.Adam(learning_rate=init_lr, decay=init_lr / epochs)\n",
    "\n",
    "model.compile(optimizer = opt, \n",
    "              loss = tf.nn.sigmoid_cross_entropy_with_logits,\n",
    "              metrics = [keras.metrics.AUC(from_logits = True, \n",
    "                                           multi_label = True)]\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "625/625 [==============================] - 451s 687ms/step - loss: 0.8268 - auc: 0.5123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arnold\\Anaconda3\\lib\\site-packages\\keras\\utils\\generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 438s 701ms/step - loss: 0.7796 - auc: 0.5212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arnold\\Anaconda3\\lib\\site-packages\\keras\\utils\\generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 440s 704ms/step - loss: 0.7581 - auc: 0.5242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arnold\\Anaconda3\\lib\\site-packages\\keras\\utils\\generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 456s 730ms/step - loss: 0.7320 - auc: 0.5263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arnold\\Anaconda3\\lib\\site-packages\\keras\\utils\\generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 427s 684ms/step - loss: 0.7034 - auc: 0.5296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arnold\\Anaconda3\\lib\\site-packages\\keras\\utils\\generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n"
     ]
    }
   ],
   "source": [
    "#use ModelCheckpoint to save the model to disk as we go along\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "batch_size = 8\n",
    "valid_batch_size = 12\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\"./model_checkpoint\")\n",
    "]\n",
    "history = model.fit(train_data,\n",
    "                    train_labels_pos,\n",
    "                    steps_per_epoch=len(train_data)//batch_size,\n",
    "                    epochs=epochs,\n",
    "                    callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise model performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 21s 787ms/step - loss: 0.6941 - auc: 0.5314\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6940694451332092, 0.5314294099807739]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(valid_data,\n",
    "               valid_labels_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['predicted_observations_acc'],\n",
    "                    name='Train'))\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['val_predicted_observations_acc'],\n",
    "                    name='Valid'))\n",
    "fig.update_layout(height=500, \n",
    "                  width=700,\n",
    "                  title='Accuracy',\n",
    "                  xaxis_title='Epoch',\n",
    "                  yaxis_title='Accuracy')\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
